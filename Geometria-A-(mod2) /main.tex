%Update 27/02/2023
% Changelog

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}     %type of document
\usepackage{lambdatex} 
\usepackage{theoremlist}


\newtheoremx{sit}{Situazione}
\newcommand\sitref[1]{\hyperref[sit: #1]{situazione \ref*{sit: #1}}}

\let\undl\underline
\newcommand{\scp}[1][\cdot,\cdot]{\left\langle #1 \right\rangle}

\makeatletter
\renewcommand*\env@cases[1][1.2]{%
  \let\@ifnextchar\new@ifnextchar
  \left\lbrace
  \def\arraystretch{#1}%
  \array{@{}l@{\quad}l@{}}%
}
\makeatother

\title{Geometria A}
\author{Davide Borra}
\date{}
\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\makeatother
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\footrulewidth}{0.4pt}

\everymath{\displaystyle}

\begin{document}

\lhead{}
\chead{}
\rhead{Indice}
\rfoot{\runauthor}

\begin{titlepage}
    \newgeometry{left=3cm, right=3cm, bottom=2cm, top =3cm} 
    \pagestyle{empty}
    \begin{center}
        \vspace*{\fill}
        \vspace{0.5cm}
        \textbf{\Huge \runtitle}\\\textsc{secondo modulo}\\\vspace{5mm}
        \textsc{\Large \runauthor}
        \vspace{5cm}
    \end{center}
    \vspace*{\fill}
    v. 1.0\\
    \rule{0.8\linewidth}{0.5mm}\\
    {\footnotesize\href{mailto:davide.borra@studenti.unitn.it}{davide.borra@studenti.unitn.it} - \href{http://davideborra.github.io}{davideborra.github.io}}
    \restoregeometry\newpage
    \thispagestyle{empty}
\end{titlepage}
    \begin{abstract}
        \centering Queste sono le note prodotte durante il secondo modulo del corso di Geometria A, tenuto dal prof.~Marco Andreatta. Il docente del corso segue il libro \say{Geometria 1} di Edoardo Sernesi (Ed. Bollati Boringhieri).
    \end{abstract}
    \tableofcontents
    \creativecommons
    \newpage
    
    
    \lhead{\runtitle}\
    \chead{}
    \rhead{\rightmark}
    \rfoot{\runauthor}

\section{Forme bilineari}
\begin{boxdef}[forma bilineare]
    Sia $V$ un $\K$-spazio vettoriale. Si dice forma bilineare una mappa lineare
    \[f:V\times V\to \K\]
    lineare rispetto ad entrambi gli argomenti, ovvero $\forall v_1, v_2, w_1, w_2 \in V$, $\forall k\in \K$
    \begin{tasks}(2)
        \task[\textbullet] $f(v_1+v_2, w_1)=f(v_1,w_1)+f(v_2,w_1)$
        \task[\textbullet] $f(v_1, w_1+w_2)=f(v_1,w_1)+f(v_1,w_2)$
        \task[\textbullet] $f(kv_1, w_1)=kf(v_1,w_1)$
        \task[\textbullet] $f(v_1, kw_1)=kf(v_1,w_1)$
    \end{tasks}
\end{boxdef}

Esistono inoltre alcune forme multilineari particolari:
\begin{boxdef}[Forme bilineari simmetriche]
    Sia $V$ un $\K$-spazio vettoriale. Una forma bilineare $f:V\times V \to \K$ si dice simmetrica se $\forall v, w\in V$
    \[f(v,w)=f(w,v)\]
\end{boxdef}
\begin{boxdef}[Forme bilineari antisimmetriche]
    Sia $V$ un $\K$-spazio vettoriale. Una forma bilineare $f:V\times V \to \K$ si dice antisimmetrica se $\forall v, w\in V$
    \[f(v,w)=-f(w,v)\]
\end{boxdef}
Data una matrice, ad essa è associata una forma bilineare del tipo $f_A(x,y)=x^t\cdot A \cdot y$. La dimostrazione del fatto che questa mappa sia bilineare segue dalla proprietà distributiva del prodotto tra matrici.
\begin{exc}\label{exc: simmetriche sse}
    Dimostrare che $f_A$ è simmetrica se e solo se $A$ è simmetrica, ovvero se e solo se $A=A^t$. 
    \qquad {\hyperref[sol: simmetriche sse]{\footnotesize Soluzione a pag. \pageref*{sol: simmetriche sse}}}
\end{exc}



\begin{shadedTheorem}[Matrice associata]
    Sia $V$ un $\K$-spazio vettoriale con dimensione $n$ finita e $\beta=\{e_1, \dots, e_n\}$ una base di $V$. Siano inoltre $f:V\times V \to \K$ una forma bilineare e $A=\left( f(e_i,e_j) \right)_{ij}\in M_{n\times n }(\K)$. Allora se $v=\sum_ia_ie_i$ e $w=\sum _jb_je_j$, si ha 
    \[f(v,w)=\left( a_1,\dots, a_n \right) \cdot A \cdot \begin{pmatrix}
        b_1\\\vdots\\ b_n
    \end{pmatrix}\]
\end{shadedTheorem}
\begin{proof}
    \[
    \begin{aligned}
        f(v,w)= f\left(\sum_ia_ie_i,\sum_ja_je_j\right)= \sum_ia_i\left( \sum_j b_jf(e_i,e_j) \right)
    \end{aligned}
    \]
    Osserviamo che 
    \[\left( a_1,\dots, a_n \right) \cdot A \cdot \begin{pmatrix}
        b_1\\\vdots\\ b_n
    \end{pmatrix} = \left( a_1,\dots, a_n \right)\begin{pmatrix}
        \sum_j b_jf(e_1,e_j)\\ \vdots \\ \sum_j b_jf(e_n,e_j)
    \end{pmatrix}  = \sum_ia_i\left( \sum_j b_jf(e_i,e_j)\right)\]
    da cui la tesi.
\end{proof}
\begin{oss}
    Scelta una base esiste una corrispondenza biunivoca tra le forme bilineari e le matrici e una corrispondenza biunivoca tra le forme bilineari simmetriche e le matrici simmetriche.
\end{oss}
\subsection{Matrici simili}
\begin{lemma}[Matrici simili]
    Siano $\beta=\{e_1, \dots, e_n\}$ e $\beta'=\{u_1, \dots, u_n\}$ due basi di $V$, $\K$-spazio vettoriale. Sia $f_V\times V\to \K$ una forma bilineare e siano $A=(f(e_i,e_j))_{ij}$ e $A=(f(u_i,u_j))_{ij}$ le matrici associate alla mappa $f$ in $\beta $ e $\beta'$ rispettivamente. Allora $\exists M=M_{\beta\beta'}(id)\in \gln{n}{\K}\ :\ B=M^t AM$
\end{lemma}
\paragraph{Notazione} se $v= \sum_ix_ie_i= \sum _ix'_iu_i$, allora $\underline x = (v)_\beta$ e $\underline x' = (v)_\beta'$. Analogamente per $w$ indicando le coordinate con $y$.
\begin{proof}
    \[\underline{x} = M\undl x'\qquad \qquad \undl y = M \undl y'\]
    da cui 
    \[f(v,w)= \undl x^tA \undl y = (M\undl x)^t A (M\undl y) = ( \undl x')^tM^tAM\undl y = ( \undl x')^t B \undl y\]
    quindi $B=M^tAM$
\end{proof}
\begin{boxdef}[Matrici congruenti]
    Due matrici $A,B\in M_{n\times n}(\K)$ si dicono congruenti se $\exists M \in \gln{n}{K}:$
    \[B=M^tAM\]
\end{boxdef}

\begin{boxdef}[Base ortogonale]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$, e $\beta$ una base di $V$. Se $i\neq j\implies b(e_i,e_j)=0$ , la base si dice diagonalizzante o ortogonale per la forma bilineare $b$. 
\end{boxdef}
\begin{oss}
    Se la base è ortogonale, allora la matrice associate $A=(b(e_i,e_j))_{ij}$ è diagonale.
\end{oss}

\begin{boxdef}[Forma quadratica associata]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$, $b: V\times V \to \K$ bilineare simmetrica. Si definisce la forma quadratica associata
    \[\funcdef{q}{V}{\K}{v}{b(v,v)}\]
\end{boxdef}
\begin{oss}
    Non è lineare.
\end{oss}
\begin{ricorda}
    Si dice forma una mappa ad un campo, non necessariamente lineare. 
\end{ricorda}
\paragraph*{Proprietà} (seguono dalla linearità di $b$)
\begin{enumerate}[label=$\roman*)$]
    \item $q(\lambda v)=\lambda^2q(v)$
    \item $2b(v,w)=q(w+v)-q(v)-q(w)$
\end{enumerate}
La proprietà (\textit{ii})è importante perché permette di definire $b$ usando $q$ e viceversa.

\begin{oss}
    Consideriamo una base $\beta=\{e_1,\dots, e_n\}$ di $V$, allora un vettore generico si esprime in coordinate come 
    \[v=\sum_ix_ie_i\]
    da cui 
    \[b(v,w)= \begin{pmatrix}
        x_1&\cdots&x_n
    \end{pmatrix} \begin{pmatrix}
        a_{ij}
    \end{pmatrix}_{ij}\begin{pmatrix}
        x_1\\\vdots\\x_n
    \end{pmatrix}=\sum_{ij}a_{ij}x_ix_j\]
    per cui in coordinate una forma quadratica è un polinomio omogeneo di secondo grado nelle variabili $x_i$ (è importante ricordare che per ipotesi $a_{ij}=a_{ji}\ \forall ij$).
\end{oss}

\begin{ex}[Consideriamo $\R^2$ con la base canonica e la forma quadratica 
    \[q(x_1,x_2)=3x_1^2-2x_1x_2-x_2^2\]]
    \[q(x_1,x_2)=3x_1^2-2x_1x_2-x_2^2=3x_1^2-x_1x_2-x_2x_1-x_2^2= \begin{pmatrix}
        x_1&x_2
    \end{pmatrix} \begin{pmatrix}
        3&-1\\-1&-1
    \end{pmatrix} \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix} \]
    Essa è quindi associata alla forma bilineare 
    \[b\left( \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix},\begin{pmatrix}
        x_1\\x_2
    \end{pmatrix}\right) = \begin{pmatrix}
        x_1&x_2
    \end{pmatrix} \begin{pmatrix}
        3&-1\\-1&-1
    \end{pmatrix} \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix} \]
\end{ex}

\begin{boxdef}[Vettori isotropi]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$ e $b(v,w)$ una forma bilineare simmetrica. Un vettore $v\in V$ si dice isotropo se 
    \[b(v,v)=q(v)=0\]
\end{boxdef}

\begin{boxdef}[Spazio ortogonale]
    Siano $V$ un $\K$-spazio vettoriale, $S\subseteq V$, $\dim V<\infty$ e $b(v,w)$ una forma bilineare simmetrica. Si definisce sottospazio perpendicolare a $S$ l'insieme 
    \[S^\perp:=\{v\in V\,|\,b(v,w)=0\ \forall w \in S\}\]
\end{boxdef}
\begin{exc}\label{exc: sottosp perp}
    Dimostrare che $S^\perp$ è un sottospazio vettoriale di $V$.\qquad {\hyperref[sol: sottosp perp]{\footnotesize Soluzione a pag. \pageref*{sol: sottosp perp}}}
\end{exc}
\begin{lemma}
    Siano $V$ un $\K$-spazio vettoriale, $S\subseteq V$, $\dim V<\infty$. Sia inoltre $v\in V$ non isotropo. Allora \[\langle v \rangle \oplus v^\perp = V\]
\end{lemma}
\begin{proof}
    Prendo un qualsiasi $w\in V$, allora sottraggo a $w$ la sua componente lungo $v$ e dimostro che appartiene a $v^\perp$, infatti 
    \[b\left( w-\frac{b(w,v)}{b(v,v)}v,v \right)= b(w,v)-b\left(\frac{b(w,v)}{b(v,v)}v,v\right)= b(w,v)-\frac{b(w,v)}{b(v,v)}b(v,v)=0\]
    quindi 
    \[ w-\frac{b(w,v)}{b(v,v)}v\in v^\perp\]
    Allora posso scrivere $w$ come la somma di un vettore in $\langle v\rangle$ e un vettore in $v^\perp$:
    \[w=\underbrace{\frac{b(w,v)}{b(v,v)}v}_{\in\langle v\rangle}+\underbrace{\left( w-\frac{b(w,v)}{b(v,v)}v \right)}_{\in v^\perp}\]
    di conseguenza \(\langle v \rangle + v^\perp = V\). Inoltre osserviamo che per costruzione di $v^\perp$, \(\langle v \rangle \cap v^\perp = \{0\}\), quindi la somma è diretta.
\end{proof}

\begin{shadedTheorem}[Diagonalizzabilità di forme bilineari]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$. Se $b$ è una forma bilineare simmetrica, allora esiste una base ortogonale per $b$, ovvero esiste una matrice $M\in \gln{n}{\K}$ tale che $M^tAM$ è diagonale.
\end{shadedTheorem}

\begin{proof}
    Se $b(v,w)$ è identicamente nulla, il teorema vale. Altrimenti possiamo supporre che $\exists v,w\in V: b(v,w)\neq 0$. Esiste quindi un vettore non isotropo, infatti 
    \begin{itemize}
        \item se $b(v,v)\neq 0$, è $v$;
        \item se $b(w,w)\neq 0$, è $w$;
        \item altrimenti, se $b(v,v)=b(w,w)=0$, $b(v+w,v+w)=b(v,v)+b(w,w)+2b(v,w)=2b(v,w)\neq 0$, per cui il vettore non isotropo é $v+w$.
    \end{itemize}
Procediamo ora per induzione su $n=\dim V$:
\begin{itemize}
    \item[$\N0)$] Se $n=1$ il teorema è vero (una matrice $1\times 1$ è diagonale)
    \item[$\N1)$] Supponiamo vero il teorema per $\dim V=n$, dimostriamo che3 vale per $\dim V = n+1$, ovvero che esiste una base diagonalizzante per ogni matrice simmetrica e per ogni spazio vettoriale di dimensione $n$. Prendiamo quindi un vettore $e_1$ non isotropo. Definiamo quindi il suo spazio perpendicolare, che (per il lemma precedente e la formula di Grassman) ha dimensione $n$. Per ipotesi induttiva esiste quindi una base ortogonale $\gamma=\{e_2, \dots, e_n\}$ di $e_1^\perp$ per $b|_{e_1^\perp}$, per cui la base cercata è \[\beta=\gamma \cup \{e_1\}=  \{e_1, \dots, e_n\}\]
\end{itemize}
\end{proof}

\begin{ex}
    [Sia $V=\R^3$ e $\beta = \{e_1,e_2,e_3\}$ la base canonica di $V$. In questa base sia $q(x,y,z)=xy+xz+yz$ una forma quadratica: diagonalizzarla.]
    Non è presente il termine $x^2$ per cui dobbiamo fare in modo di ricavarlo. Poniamo quindi
    \[\begin{cases}
        x'=x\\
        y'=y+x\\
        z'=z
    \end{cases}\qquad \qquad \begin{cases}
        x=x'\\
        y=y'-x'\\
        z=z'
    \end{cases}\]
    Sostituiamo e completiamo il quadrato
    \[q(x',y',z')=x'(y'-x')+x'z'+(y'-x')z'= -{x'}^2+x'y'+y'z'=-\left(x'-\frac{1}{2}y'\right)^2+\frac{1}{4}{y'}^2+y'z'\]
    Cambiamo coordinate
    \[\begin{cases}
        x''=x'-\frac{1}{2}y'\\
        y''=y'\\
        z''=z'
    \end{cases}\qquad \qquad \begin{cases}
        x'=x''+\frac{1}{2}y''\\
        y'=y''\\
        z'=z''
    \end{cases}\]
    \[q(x'',y'',z'')=-{x''}^2+\frac{1}{4}{y''}^2+y''z''=-{x''}^2+\left( \frac{1}{2}y''+z'' \right)^2-{z''}^2\]
    \[\begin{cases}
        x'''=x''\\
        y'''=\frac{1}{2}y''+z''\\
        z'''=z''
    \end{cases}\qquad \qquad \begin{cases}
        x''=x'''\\
        y''=2y'''-2z'''\\
        z''=z'''
    \end{cases}\]
    \[q(x''',y''',z''')=-{x'''}^2+{y'''}^2-{z'''}^2\]

    Ricaviamo ora le matrici di cambio di base:
    \[\begin{aligned}
        [I]\qquad \qquad &
        \begin{pmatrix}
            1&0&0\\
            -1&1&0\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x'\\y'\\z'
        \end{pmatrix}= \begin{pmatrix}
            x\\y\\z
        \end{pmatrix}\\
    [II]\qquad \qquad &
        \begin{pmatrix}
            1&\frac{1}{2}&0\\
            0&1&0\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x''\\y''\\z''
        \end{pmatrix}= \begin{pmatrix}
            x'\\y'\\z'
        \end{pmatrix}\\
    [III]\qquad \qquad &
        \begin{pmatrix}
            1&0&0\\
            0&2&-2\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x'''\\y'''\\z'''
        \end{pmatrix}= \begin{pmatrix}
            x''\\y''\\z''
        \end{pmatrix}
    \end{aligned}
    \]
    quindi
    \[\underbrace{\begin{pmatrix}
        1&0&0\\
        -1&1&0\\
        0&0&1
    \end{pmatrix}
    \begin{pmatrix}
        1&\frac{1}{2}&0\\
        0&1&0\\
        0&0&1
    \end{pmatrix}
    \begin{pmatrix}
        1&0&0\\
        0&2&-2\\
        0&0&1
    \end{pmatrix}}_{M}\begin{pmatrix}
        x'''\\y'''\\z'''
    \end{pmatrix}= \begin{pmatrix}
        x\\y\\z
    \end{pmatrix}\]
    da cui
    \[{\renewcommand{\arraystretch}{2}
        M^t\cdot\begin{pmatrix}
        0&\frac{1}{2}&\frac{1}{2}\\
        \frac{1}{2}&0&\frac{1}{2}\\
        \frac{1}{2}&\frac{1}{2}&0
    \end{pmatrix}\cdot M}=\begin{pmatrix}-1&0&0\\0&1&0\\0&0&-1\end{pmatrix}\]
\end{ex}

\begin{shadedTheorem}[Sylvester I]
    Sia $\K$ un campo algebricamente chiuso. Sia $V$ un $\K$-spazio vettoriale con $\dim V=n\geq 1$. Sia $b:V\times V\to \K$ una forma bilineare simmetrica. Allora esiste una base 
    \[\beta=\{u_1, \dots, u_n\}\]
    tale che 
    \[\begin{array}{rcl}
        i\neq j & \implies  &b(u_i,u_j)=0;\\
        1\leq i\leq r\leq n &\implies &b(u_i,u_i)=1;\\
        r<i\leq n& \implies & b(u_i,u_i)=0.
    \end{array}\]
    Equivalentemente la forma quadratica rispetto a $\beta$ è 
    \[q(x_1, \dots, x_n)=x_1^2+\dots+ x_r^2\]
\end{shadedTheorem}
\begin{proof}
    Per il teorema precedente, esiste una abse ortogonale $\gamma=\{e_i\}_i$. Riordinando possiamo supporre che 
    \[\begin{array}{rcl}
        1\leq i\leq r\leq n &\implies &b(e_i,e_i)\neq 0;\\
        r<i\leq n& \implies & b(e_i,e_i)=0.
    \end{array}\]
    Poniamo 
    \[\begin{cases}
        u_i:=\frac{e_i}{\sqrt{b(e_i,e_i)}}& i=1,\dots, r\\
        u_i:=e_i& i=r+1, \dots, n
    \end{cases}\]
    Otteniamo che $\beta=\{u_i\}_i$ è la base cercata.
\end{proof}

\begin{shadedTheorem}[Sylvester II]
    Siano $V$ un $\R$-spazio vettoriale e $b:V\times V\to \R$ una forma bilineare simmetrica. Allora esiste una base 
    \[\beta=\{u_2, \dots, u_n\}\]
    tale che 
    \[\begin{array}{rcl}
        i\neq j & \implies  &b(u_i,u_j)=0;\\
        1\leq i\leq r\leq t\leq n &\implies &b(u_i,u_i)=1;\\
        1\leq r< i\leq t\leq n &\implies &b(u_i,u_i)=-1;\\
        t<i\leq n& \implies & b(u_i,u_i)=0.
    \end{array}\]
\end{shadedTheorem}
La dimostrazione è analoga al teorema precedente ponendo 
\[\begin{cases}
    u_i:=\frac{e_i}{\sqrt{|b(e_i,e_i)|}}& i=1,\dots, r\\
    u_i:=e_i& i=r+1, \dots, n
\end{cases}\]
e riordinando opportunamente.

Un'osservazione interessante è che il numero di 0, di 1 e di -1 non dipende dalla scelta di una base ortogonale.
\begin{shadedTheorem}[Rango]
    ($V$ $\K$-spazio vettoriale, $\dim V=n$) Data una forma bilineare simmetrica $b:V\times V \to \K$ e una sua base diagonalizzante $\beta=\{e_i\}_i$, il numero di vettori $e_i$ tali che $b(e_i,e_i)=0$ non dipende dalla scelta della base diagonalizzante. Esso è uguale al rango di ogni matrice associata.
\end{shadedTheorem}
\begin{proof}
    Ricordiamo il seguente lemma:
    \begin{lemma}
        Siano $A\in M{n\times m}(\K)$, $N\in \gln{n}{\K}$ e $M\in \gln{m}{\K}$, allora $\rk\left( NAM \right)=\rk A$.
    \end{lemma}
    Siano $\beta$ e $\beta'$ due basi, $M=M_{\beta\beta'}(Id)$, $A=M_\beta(b)$ e $B=M_{\beta'}(b)$. Allora per dimostrazione precedente si ha $B=M^tAM$. Per il lemma appena enunciato segue che, siccome $M\in \gln{n}{\K}$, $\rk B=\rk A$.
    Supponiamo che esistano due basi $\beta=\{e_i\}_i$ e $\beta'=\{u_i\}_i$ diverse tali che (numeri di 1 e -1 diversi).
    \begin{itemize}
        \item \[\begin{array}{rcl}
            i\neq j & \implies  &b(e_i,e_j)=0;\\
            1\leq i\leq t\leq r\leq n &\implies &b(e_i,e_i)=1;\\
            1\leq t< i\leq r\leq n &\implies &b(e_i,e_i)=-1;\\
            r<i\leq n& \implies & b(e_i,e_i)=0.
        \end{array}\]
        \item \[\begin{array}{rcl}
            i\neq j & \implies  &b(u_i,u_j)=0;\\
            1\leq i\leq s\leq r\leq n &\implies &b(u_i,u_i)=1;\\
            1\leq s< i\leq r\leq n &\implies &b(u_i,u_i)=-1;\\
            r<i\leq n& \implies & b(u_i,u_i)=0.
        \end{array}\]
    \end{itemize}
    con $s\neq t$. È lecito supporre che $t>s$ (altrimenti scegliamo al posto di $U$ e $W$ i rispettivi complementi diretti a $V$). Siano $U=\langle e_1, \dots, e_t\rangle$ e $W=\langle u_{s+1},\dots, u_n\rangle$. Segue quindi che $\dim U=t$ e $\dim W=n-s$, da cui 
    \[\dim U+\dim W=n-s+t>n,\]
    quindi per la formula di Grassman segue che $\dim U\cap V>0$. Prendiamo quindi un vettore non nullo $v\in U\cap V$.
    \[v=\sum_{i=1}^ta_ie_i=\sum_{i=s+1}^nb_iu_i\] 
    per cui
    \[\begin{aligned}
        b(v,v)&=\sum_{i=1}^t\underbrace{b(e_i,e_i)}_1a_i^2= \sum_{i=1}^ta_i^2>0\\
        &=\sum_{i=s+1}^n\underbrace{b(u_i,u_i)}_{-1}b_i^2= -\sum_{i=s+1}^nb_i^2<0\\
    \end{aligned}.\]
    Si presenta quindi un assurdo, per cui deve essere che $s=t$.
\end{proof}
\begin{boxdef}[Rango]
    ($V$ $\K$-spazio vettoriale, $\dim V=n$) Si definisce rango di una forma bilineare simmetrica $b:V\times V\to \K$ il numero di vettori $e_i$ di una base diagonalizzante tali che $b(e_i,e_i)\neq 0$. Esso si indica $\rk b$. Se $\rk b=\dim V$, allora la forma si dice non degenere.
\end{boxdef}
\begin{boxdef}[Segnatura]
    Siano $V$ un $\R$-spazio vettoriale, $b:V\times V\to \K$ una forma bilineare simmetrica e $\beta=\{e_i\}_i$ una base diagonalizzante normalizzata. 
    Allora si definiscono
    \begin{itemize}
        \item \textbf{indice di positività:} $t=\#\{e\in \beta\,|\, b(e,e)=1\}$
        \item \textbf{indice di negatività:} $s=\#\{e\in \beta\,|\, b(e,e)=-1\}$
    \end{itemize}
    La coppia $(t,s)$ è detta segnatura di $b$.
\end{boxdef}
\begin{boxdef}[Forme definite positive/negative]
    ($V$ $\R$-spazio vettoriale, $\dim V=n$) Una forma quadratica $q$ su $V$ si dice:
    \begin{itemize}
        \item \textbf{definita positiva} se $q(v)>0\ \ \forall\,v\in V\setminus\{0\} $
        \item \textbf{definita negativa} se $q(v)<0\ \ \forall\,v\in V \setminus\{0\}$
        \item \textbf{semidefinita positiva} se $q(v)\geq0\ \ \forall\,v\in V $
        \item \textbf{semidefinita negativa} se $q(v)\leq0\ \ \forall\,v\in V $ 
    \end{itemize}
\end{boxdef}
In particolare lo è se lo è su una base.
\begin{lemma}[Finestra sul mondo duale]
    Siano $V$ un $\K$-spazio vettoriale, $b:V\times V\to \K$ una forma bilineare simmetrica non degenere, allora esiste un isomorfismo
    \[\begin{array}{ccccccl}            \psi : & V & \overset{\sim}{\longrightarrow}& V^\vee \\                 & v & \longmapsto     & \psi(v) : & V & \longrightarrow & \mathbb{K} \\&&&                 & w & \longmapsto     &\psi(v)(w):=b(v,w)       \end{array}\]
\end{lemma}
\begin{exc}\label{exc: lemma finestra}
    La dimostrazione del lemma precedente è lasciata per esercizio. \qquad {\hyperref[sol: lemma finestra]{\footnotesize Soluzione a pag. \pageref*{sol: lemma finestra}}}
\end{exc}
%\vartriangleleft per sottospazi vettoriali

\section{Geometria Euclidea}
\subsection{Prodotto scalare}
\begin{boxdef}[Prodotto scalare]
    Sia $V$ un $\R$-spazio vettoriale. Una forma $b:V\times V\to \R$ \begin{enumerate}[label=$\roman*)$]
        \item bilineare 
        \item simmetrica
        \item definita positiva, ovvero 
        \[b(v,v)\geq 0\qquad  \land\qquad b(v,v)=0\ \Harr\ v=0\]
    \end{enumerate}
    si dice prodotto scalare su $V$, e si indica $\langle v,w \rangle$ o $v\cdot w$
\end{boxdef}
Per il teorema di Sylvester e per il fatto che il prodotto scalare è una forma definita positiva, segue che esiste una base $\beta=\{e_i\}_i$ di $V$ tale che
\[\scp[e_i,e_j]=\delta_{ij}\] 
ovvero una base ortonormale (ortogonale e normalizzata). Inoltre siccome la mappa è definita positiva, non esistono vettori isotropi.

\begin{lemma}[Disuguaglianza di Cauchy-Schwarz]
    Sia $\scp$ un prodotto scalare su $V$ ($\R$-spazio vettoriale). Allora $\forall v,w\in V$, vale \[\scp[v,w]^2\leq \scp[v,v]\cdot \scp[w,w]\]
\end{lemma}
\begin{proof}\footnote{Analisi A - 2Lez(2) 02/03/2023}
    Se $x=0\lor y=0$ è ovvio. Supponiamo quindi $x\neq 0\lor y\neq 0$. Allora per linearità del prodotto scalare
    \[\scp[\lambda x-y,\lambda x-y]=\lambda^2\scp[x,x]-2\lambda \scp[x,y]+\scp[y,y].\]
    Osserviamo che a primo membro per definizione di prodotto scalare abbiamo una quantità positiva, di conseguenza anche a secondo membro. In particolare un polinomio in $\lambda$ è positivo $\forall \lambda$ se e solo se ha discriminante negativo, di conseguenza
    \[\Delta=4\scp[x,y]^2-4\scp[x,x]\scp[y,y]\leq 0 \quad \Harr\quad \scp[x,y]^2\leq \scp[x,x]\scp[y,y]\]
\end{proof}

\begin{boxdef}[Spazio euclideo]
    Uno spazio vettoriale $V$ su $\R$ con un prodotto scalare $\scp$ si dice spazio euclideo o pre-hilbertiano. Si indica $(V,\scp)$.
\end{boxdef}

Un prodotto scalare induce anche una norma, per cui uno spazio euclideo è automaticamente uno spazio normato definendo

\begin{boxdef}[Norma]
    Uno spazio euclideo è uno spazio normato definendo la norma di un vettore come 
    \[\|v\|=\sqrt{\scp[v,v]}\in [0;+\infty[\]
\end{boxdef}
\paragraph{Proprietà}
\begin{enumerate}[label=$\roman*)$]
    \item $\|v\|\geq 0\forall v \in V \quad \land \quad \|v\|=0\ \Harr\ v=0$
    \item (omogeneità) $\|\lambda v\|=|\lambda|\|v\|$
    \item (disuguaglianza triangolare) $\|v\|+\|w\|\geq \|v+w\|$
\end{enumerate}
Osserviamo inoltre che nel punto (\textit{iii}) i due membri sono uguali solo se i due vettori sono multipli l'uno dell'altro.

Il prodotto scalare permette inoltre di definire il concetto di angolo compreso tra due vettori. La definizione è corretta in quanto la disuguaglianza di Cauchy-Schwarz garantisce che il secondo membro sia in valore assoluto sempre minore di 1.
\begin{boxdef}[Angolo compreso tra due vettori]
    Siano $v,w \in (V,\scp)$. Si definisce allora l'angolo convesso non orientato compreso tra $v$ e $w$ l'angolo $\vartheta\in [0;\pi]$ tale che 
    \[\cos \vartheta = \frac{\scp[v,w]}{\|v\|\cdot \|w\|}\]
\end{boxdef}
\begin{oss}
    $\scp[v,w]=0\ \Harr \ \widehat{vw}=\vartheta = \frac{\pi}{2}$, ovvero la definizione usuale di perpendicolarità coincide con quella derivante dal prodotto scalare.
\end{oss}
\subsection{Teorema di ortogonalizzazione di Gram-Schmidt}

\begin{shadedTheorem}[Ortogonalizzazione - Gram-Schmidt]
    Sia $(v_i)_{i\in I}$ una successione (finita o infinita) di vettori nello spazio euclideo $V$ e $I$ un insieme di indici. Allora esiste una successione $(w_i)_{i\in I}$ di vettori di $V$ tali che:
    \begin{enumerate}[label=$\roman*)$]
        \item $\forall k\geq 1$, $\langle v_1, \dots, v_k\rangle= \langle w_1, \dots, w_k\rangle$
        \item $w_1, \dots, w_k$ sono a due a due ortogonali.
    \end{enumerate}
    La successione $(w_i)_{i\in I}$ è unica a meno di multipli scalari.
\end{shadedTheorem}

\begin{proof}
    Procediamo per induzione su $k$
    \begin{enumerate}
        \item[$\N0)$] Se $k=1$ non c'è niente da dimostrare.
        \item[$\N1)$] Supponiamo che il teorema valga per $k-1$, ovvero che $\exists w_1, \dots, w_{k-1}$ che soddisfano (\textit{i}) e (\textit{ii}). Definiamo 
        \[w_k=v_k-\sum_{i=1}^{k-1}\frac{\scp[v_k,w_i]}{\scp[w_i,w_i]}w_i\]
        sommando solo sugli $i$ tali che $w_i\neq 0$. Stiamo rimuovendo $v$ le sue proiezioni su tutti gli altri vettori $w_i$.

        Osserviamo ora che $\forall j<k$ (ricordando che per (\textit{ii}), $\forall i < k, i\neq j\implies \scp[w_i,w_j]=0$)
        \[\scp[w_k,w_j]=\scp[v_k-\sum_{i=1}^{k-1}\frac{\scp[v_k,w_i]}{\scp[w_i,w_i]}w_i, w_j] = \scp[v_k,w_j]-\sum_{i=1}^{k-1}\frac{\scp[v_k,w_i]}{\scp[w_i,w_i]}\scp[w_i,w_j] = \scp[v_k,w_j]-\frac{\scp[v_k,w_j]}{\cancel{\scp[w_j,w_j]}}\cancel{\scp[w_j,w_j]}=0,\]
        il che dimostra che vale (\textit{ii}). Osserviamo ora che per costruzione 
        \[w_k\in \langle w_i, \dots, w_{k-1}, v_k \rangle\]
        in quanto ne è combinazione lineare. Inoltre per ipotesi induttiva
        \[\langle w_i, \dots, w_{k-1}, v_k \rangle=\langle v_1, \dots, v_k\rangle\]
        di conseguenza per il teorema di Steinitz posso scambiare $v_k$ con $w_k$ ed ottenere comunque un insieme di generatori, quindi 
        \[\langle w_i, \dots, w_{k-1},\undl{w_k} \rangle=\langle v_1, \dots, v_k\rangle\]
    \end{enumerate}
\end{proof}

\begin{boxdef}[Insiemi ortogonali e ortonormali]
    Sia $(V,\scp)$ uno spazio euclideo. Un insieme $\{v_i\}_i\subset V$ si dice ortogonale se 
    \begin{enumerate}[label=$\roman*)$]
        \item $v_i\neq 0\ \forall i$
        \item $i\neq j \implies \scp[v_i,v_j]=0$
    \end{enumerate}
    si dice inoltre ortonormale se 
    \begin{enumerate}[label=$\roman*)$, resume]
        \item $\scp[v_i,v_i]=1\ \forall i$
    \end{enumerate}
\end{boxdef}

\begin{prop}
    Sia $(V,\scp)$ uno spazio euclideo. Un insieme ortogonale $\{v_i\}_{1\leq i\leq r}\subset V$ con $r\leq \dim V$ è linearmente indipendente.
\end{prop}
\begin{proof}
    Prendiamo una combinazione lineare nulla $\sum_{i=1}^ra_iv_i=0$. Osserviamo che $\forall j$
    \[0=\scp[\sum_{i=1}^ra_iv_i,v_j] = \sum_{i=1}^ra_i\scp[v_i,v_j]=a_j\underbrace{\scp[v_j,v_j]}_{\neq 0}=0\]
    per cui, per la legge di annullamento del prodotto, $a_j=0$.
\end{proof}
\begin{ex}
    [Consideriamo $\R^4$ con la base canonica e il prodotto scalare standard. Siano $v_1=(0,1,0,1)$, $v_2=(2,1,0,1)$, $v_3=(-1,0,0,1)$ e $v_4=(0,0,1,0)$. Ortogonalizzare questo insieme di vettori]
    Iniziamo ponendo $w_1:=v_1=(0,1,0,1)$. Ora ricaviamo di conseguenza gli altri vettori:
    \[w_2=v_2-\frac{\scp[v_2,w_1]}{\scp[w_1,w_1]}=\begin{pmatrix}
        2\\1\\0\\1
    \end{pmatrix}-\frac{2}{2}\begin{pmatrix}
        0\\1\\0\\1
    \end{pmatrix}=\begin{pmatrix}
        2\\0\\0\\0
    \end{pmatrix}\]
    \[w_3=v_3-\frac{\scp[v_3,w_1]}{\scp[w_1,w_1]}-\frac{\scp[v_3,w_1]}{\scp[w_1,w_1]}= \begin{pmatrix}
        -1\\0\\01
    \end{pmatrix}-\frac{1}{2}\begin{pmatrix}
        0\\1\\0\\1
    \end{pmatrix}-\frac{-2}{4}\begin{pmatrix}
        2\\0\\0\\0
    \end{pmatrix}=\begin{pmatrix}
        0\\-\textstyle\frac{1}{2}\\0\\\textstyle\frac{1}{2}
    \end{pmatrix}\]

    \[w_4=v_4-\frac{\scp[v_4,w_1]}{\scp[w_1,w_1]}-\frac{\scp[v_4,w_2]}{\scp[w_2,w_2]}-\frac{\scp[v_4,w_1]}{\scp[w_3,w_3]}= \begin{pmatrix}
        0\\0\\1\\0
    \end{pmatrix}-0\begin{pmatrix}
        0\\1\\0\\1
    \end{pmatrix}-0\begin{pmatrix}
        2\\0\\0\\0
    \end{pmatrix}-0\begin{pmatrix}
        0\\-\textstyle\frac{1}{2}\\0\\\textstyle\frac{1}{2}
    \end{pmatrix}=\begin{pmatrix}
        0\\0\\1\\0
    \end{pmatrix}\]
\end{ex}

\begin{prop}
    Siano $(V,\scp)$ uno spazio euclideo e $W\sottosp V$. Allora $V=W\oplus W^\perp$.
\end{prop}
\begin{proof}
    Prendiamo una base ortonormale di $V$ $\{e_1, \dots, e_n\}$ tale che $W=\langle e_1,\dots, e_r\rangle$. Allora 
    \[v=\sum_{i=1}^na_ie_i=\underbrace{\sum_{i=1}^ra_ie_i}_{\in W}+\sum_{i=r+1}^na_ie_i.\]
    Osserviamo che $\forall 1\leq j\leq r$, 
    \[\scp[\sum_{i=r+1}^na_ie_i, e_j]=0\]
    per ipotesi, di conseguenza $\scp[\sum_{i=r+1}^na_ie_i\in W^\perp]$. Da ciò segue che $V=W+W^\perp$. Siccome non esistono vettori isotropi, $W \cap W^\perp=\{0\}$. La somma è quindi diretta.
\end{proof}

\subsection{Prodotto esterno}

\begin{boxdef}[Prodotto esterno]
    Consideriamo un $\R$-spazio vettoriale $V$ di dimensione 3. Si definisce prodotto esterno la mappa 
    \[\funcdef{\wedge}{\R^3\times \R^3}{\R^3}{\left(\begin{pmatrix}
        x_1\\x_2\\x_3
    \end{pmatrix}, \begin{pmatrix}
        y_1\\y_2\\y_3
    \end{pmatrix}\right)}{\begin{pmatrix}
        x_1\\x_2\\x_3
    \end{pmatrix}\wedge \begin{pmatrix}
        y_1\\y_2\\y_3
    \end{pmatrix}:=\begin{pmatrix}
        x_2y_3-x_3y_2\\x_3y_1-x_1y_3\\x_1y_2-y_2x_1
    \end{pmatrix}}\]
\end{boxdef}
\paragraph{Proprietà}
\begin{enumerate}[label=$\roman*$)]
    \item $v\wedge w\neq 0\ \Harr\ \nexists k : v\neq kw$
    \item $\scp[v\wedge w, v]= \scp[v\wedge w, w]=0$
    \item $v\wedge w = -w\wedge v$
    \item $\|v\wedge w\|^2=\|v^2\|\cdot\|w^2\|-\scp[v,w]^2$
    \item $(\lambda v)\wedge w = v\wedge (\lambda w)= \lambda (v\wedge v)$
    \item $v\land (w\land u)=(v\land w)\land u$
\end{enumerate}
\begin{proof} Dimostriamo solo (\textit{ii}). Osserviamo che 
    \[0=\begin{vmatrix}
        x_1&x_2&x_3\\
        x_1&x_2&x_3\\
        y_1&y_2&y_3\\
    \end{vmatrix}=x_1(x_2y_3-x_3y_2)+x_2(x_3y_1-x_1y_3)+x_3(x_1y_2-y_2x_1)=\scp[v, v\wedge w]\]
    
\end{proof}

\subsection{Spazi affini}

\begin{boxdef}[Spazio Affine]
    $\mathbb{K}$ campo, $V ~~ \mathbb{K}$-spazio vettoriale.
    Uno \undl{spazio affine con giacitura $V$} è un insieme $\mathbb{A}$ munito di una mappa
    \[\funcdef{\overline{\:\cdot\:\cdot\:}}{\A\times \A}{V}{(P,Q)}{\overline{PQ}}\]
    tale che 
    \begin{enumerate}[label=$\roman*)$]
        \item $\forall P\in \A, \forall v\in V, \exists!Q\in \A : \overline{PQ}=v$;
        \item $\forall P,Q,R\in \A, \overline{PQ}+\overline{QR}=PR$.
    \end{enumerate}
    Gli elementi di $\A$ si dicono \undl{punti}.
\end{boxdef}

Uno spazio vettoriale è uno spazio affine su sé stesso definendo 
\[\funcdef{\overline{\:\cdot\:\cdot\:}}{V\times V}{V}{(v,w)}{w-v}\]

\begin{exc}\label{exc: sottosp aff-vett}
    Dimostrare che con questa definizione uno spazio vettoriale è uno spazio affine su sé stesso, ovvero che soddisfa (\textit{i}) e (\textit{ii}). \qquad {\hyperref[sol: sottosp aff-vett]{\footnotesize Soluzione a pag. \pageref*{sol: sottosp aff-vett}}}
\end{exc}

\begin{oss}
    $\overline{PP}=\undl{0}$, infatti da (\textit{ii}) segue che $\overline{PP}+\overline{PP}=\overline{PP}$.
\end{oss}

\begin{boxdef}[Dimensione di uno spazio affine]
    Sia $\mathbb{A}$ uno spazio affine con giacitura $V$ ($\mathbb{K}$-spazio vettoriale). Si definisce la dimensione di $\mathbb{A}$ come la dimensione di $V$.
    \[
    \dim \mathbb{A}:=\dim V
    \]
\end{boxdef}

\begin{boxdef}[Traslazione]
    Si definisce traslazione una mappa \[\begin{array}{cccl}t:& \mathbb{A}\times V&\longrightarrow &\mathbb{A}\\
        &(P,v)&\longmapsto&Q=t(P,v)=:P+v\end{array}\]
        dove $P+v$ si chiama traslato di $P$ per $v$ tale per cui $\overline{PQ}=v$
\end{boxdef}

\paragraph*{Proprietà} Segue direttamente dagli assiomi dello spazio affine che 
\begin{enumerate}[label=$\roman*)$]
    \item $t(P,\underline{0})=P$
    \item $t(t(P,u),v)=t(P,u+v)$
    \item $\forall P,Q\in \mathbb{A}~~~\exists!v:Q=t(P,v)$
\end{enumerate}


\subsection{Sistemi di riferimento affini}
\begin{boxdef}[Sistema di riferimento affine]
    Sia $\mathbb{A}$ uno spazio affine con giacitura $V$ ($\mathbb{K}$-spazio vettoriale) con $\dim \mathbb{A}=n<\infty$.
    Un sistema di riferimento affine è dato da un punto $O$ detto \undl{origine} e una base $\beta$ di $V$.
\end{boxdef}
Grazie ad un sistema di riferimento è possibile esprimere i punti attraverso coordinate, come avevamo precedentemente fatto per i vettori. La differenza sta nel fatto che di per sé uno spazio affine non presenti un punti privilegiato, come può essere lo $0$ nello spazio vettoriale, per cui per poter esprimere i punti come coordinate bisogna sceglierne uno.

\subsection{Sottospazi affini}
\begin{boxdef}
    Siano $\mathbb{A}$ spazio affine con giacitura $V$, $P\in \mathbb{A}$ un punto e $W\subseteq V$ un sottospazio vettoriale. Allora si dice \undl{sottospazio affine} di $\mathbb{A}$ l’insieme
    $$
    S=P+W=\{Q\in \A\:|\: \overline{PQ}\in W\}
    $$
\end{boxdef}
\begin{lemma}
    Con la notazione precedente, $P+W$ è uno spazio affine con giacitura $W$
\end{lemma}
\begin{proof}
    Dobbiamo dimostrare che presi qualsiasi $A, B\in S$, allora $\overline{AB}\in W$. Per $(ii)$ si ha che $\overline{AB}= \overline{AP}+\overline{PB}\in W$ per definizione di S.  
\end{proof}
Vediamo ora una serie di esempi di sottospazi affini:
\begin{ex}
    [Sia $\A^2=\R^2$ con il sistema di riferimento canonico. Trovare la equazioni parametriche e cartesiane della retta $S$ passante per $P=(-1,2)$ con direzione (giacitura) $w=(1,3)$.] 

    Il fatto che un punto appartenga a $S$ significa che è della forma $P+kw$. Di conseguenza
    \[\begin{cases}
        x=-1+t\\
        y=2-3t
    \end{cases}\qquad \Harr\qquad 3x+y=1\]
    Per ottenere la forma cartesiana (a destra) si può anche procedere applicando la definizione: i vettori $PQ$ e $w$ devono essere linearmente dipendenti, ovvero la matrice seguente deve avere rango 2, quindi 
    \[\begin{vmatrix}
        x+1&y+2\\
        1&3
    \end{vmatrix}=0\]
\end{ex}

\begin{ex}[Generalizziano l'esempio seguente considerando lo spazio affine $\A$ di dimensione $n$ con il sistema di riferimento canonico e scriviamo le equazioni del sottospazio affine $S$ passante per $P=(p_1, \dots, p_n)$ con giacitura $W=\langle w_1,\dots, w_r\rangle$, dove $w_i=(w_{i1},\dots,w_{in})$ e i $w_i$ sono linearmente indipendenti.]
\paragraph*{Equazioni cartesiane}
Per determinare l'equazione cartesiana abbiamo bisogno che un vettore generico $\overline{PQ}$ sia linearmente dipendente dagli altri, per cui che il rango della matrice seguente sia pari a $r$
\[\rk\begin{pmatrix}
    x_1-p_1&x_2-p_2&\dots&x_n-p_n\\
    w_{11}&w_{12} & \dots & w_{1n}\\
    \vdots&\vdots&\ddots&\vdots\\
    w_{r1}&w_{r2} & \dots & w_{rn}\\
\end{pmatrix}=r\]

Siccome i vettori sono linearmente indipendenti, deve esistere un minore di ordine $r$ non nullo. Per avere che la matrice ha rango $r$ bisogna quindi imporre che tutti i minori orlati abbiano determinante nullo. Otteniamo quindi $n-r$ equazioni.
\paragraph*{Equazioni parametriche}
Per determinare le equazioni parametriche basta semplicemente procedere come prima, ricordando che $Q\in S \Harr Q=P+v $ con $v\in W$, ovvero $v=\sum_it_iw_i$. Segue quindi che (separando nelle componenti):
\[\begin{cases}
    x_1=p_1+t_1w_{11}+t_2w_{21}+\dots+t_rw_{r1}\\
    x_2=p_1+t_1w_{12}+t_2w_{22}+\dots+t_rw_{r2}\\
    \ \vdots\\
    x_n=p_1+t_1w_{1n}+t_2w_{2n}+\dots+t_rw_{rn}\\
\end{cases}\]
\end{ex}

\begin{boxdef}[Sottospazi paralleli]
    Siano $\A$ uno spazio affine su $V$ ($\K$-spazio vettoriale), $S$ e $T$ due sottospazi affini con giacitura rispettivamente $U$ e $W$. Essi si dicono paralleli se $U\sottosp W$ o $W\sottosp U$.
\end{boxdef}

\subsection{Posizioni reciproche di di sottospazi affini}
\begin{sit}\label{sit: 1}
    Sia $\A$ uno spazio affine su $V$ ($\K$-spazio vettoriale) con il sistema di riferimento canonico $(O,\{e_1,\dots, e_n\})$ e siano $P=(p_1,\dots, p_n)$ e $R=(r_1, \dots, r_n)$ due punti di $\A$. Siano inoltre $S$ il sottospazio affine per $P$ con giacitura $W=\langle w_1, \dots, w_r\rangle$ e T il sottospazio affine per $R$ con giacitura $U=\langle w_1, \dots, u_t\rangle$
\end{sit}

Osserviamo che l'intersezione tra due sottospazi affini è un sottospazio affine tale che
\[S\cap T =\{Q\in \A\:|\:Q\in S \land Q\in T\}\]
ovvero $Q=(x_1, \dots, x_n)$ soddisfa sia le equazioni che definiscono $S$ che le equazioni che definiscono $T$. Otteniamo in questo modo un sistema lineare in $r+t$ equazioni che può essere
\begin{itemize}
    \item incompatibile, quindi $S\cap T=\varnothing$
    \item compatibile con rango $\rho$ tale che $\max\{n-r,n-t\}\leq \rho\leq (n-r)+(n-t)$.
\end{itemize}
\begin{lemma}[Dimensione dell'intersezione]
    Nella \sitref{1}, si hanno due possibilità:
    \begin{itemize}
        \item $S\cap T=\varnothing$
        \item $\dim S+\dim T -\dim \A \leq \dim S\cap T \leq \min\{\dim S,\dim T\} $
    \end{itemize}
\end{lemma}

\begin{proof}
    Da quanto detto prima segue che $\dim S\cap T = n-\rho = n-(n-r)-(n-t)=r+t-n$.
\end{proof}

\begin{oss}
    Nella \sitref{1}, ad $S$ è associato un sistema del tipo 
    \[\begin{cases}
        a_{11}x_1+a_{12}x_2+\dots+a_{1n}x_n+b_1=0\\
        ~\vdots\\
        a_{(n-r),1}x_1+a_{(n-r),2}x_2+\dots+a_{(n-r),n}x_n+b_{(n-r)}=0,\\
    \end{cases}\]
    allora la giacitura $W$ di $S$ è determinata il sistema omogeneo associato, ovvero 
    \[S:~\rk\begin{vmatrix}
        x_1-p_1&\cdots&x_n-p_n\\
        w_{11}&\cdots& w_{1n}\\
        \vdots& \ddots & \vdots \\
        w_{r1}&\cdots& w_{rn}\\
    \end{vmatrix}=r\qquad \qquad W:~\rk\begin{vmatrix}
        x_1&\cdots&x_n\\
        w_{11}&\cdots& w_{1n}\\
        \vdots& \ddots & \vdots \\
        w_{r1}&\cdots& w_{rn}\\
    \end{vmatrix}=r\]
\end{oss}

\paragraph{Un caso particolare: due rette in $\A^3$}
Siano $r$ e $s$ due rette generiche nello spazio affine definite come segue:
\[r:\ \begin{cases}
    a_1x+b_1y+c_1z+d_1=0\\
    a_2x+b_2y+c_2z+d_2=0
\end{cases}\qquad \qquad s:\ \begin{cases}
    a_3x+b_3y+c_3z+d_3=0\\
    a_4x+b_4y+c_4z+d_4=0
\end{cases}\]
possono verificarsi quattro casi:
\begin{itemize}
    \item le due rette sono coincidenti (i.e. si intersecano in infiniti punti);
    \item le due rette sono incidenti (i.e. si intersecano in un punto);
    \item le due rette sono parallele (i.e. non si intersecano e hanno la stessa giacitura);
    \item le due rette sono sghembe (i.e. non si intersecano e hanno giaciture diverse).
\end{itemize}
\begin{boxdef}[Rette complanari e sghembe]
    Siano $r$ e $s$ due rette nello spazio $\A^3$, esse si dicono
    \begin{itemize}
        \item \textbf{complanari} se $r\cap s\neq \varnothing$ o $r\parallelo s$
        \item \textbf{sghembe} se non sono complanari, ovvero se non si intersecano e non sono parallele.
    \end{itemize}
\end{boxdef}

\begin{prop}
    $r$ e $s$ come definite precedentemente sono complanari se e solo se 
    \[\begin{vmatrix}
        a_1&b_1&c_1&d_1\\
        a_2&b_2&c_2&d_2\\
        a_3&b_3&c_3&d_3\\
        a_4&b_4&c_4&d_4\\
    \end{vmatrix}=0\]
\end{prop}
\begin{proof}
    Chiamiamo $C$ la matrice dei coefficienti e $D$ la matrice dei termini noti. Sia inoltre $A$ la matrice $(C|D)$ che si ottiene orlando $C$ con $D$.
    \begin{itemize}
        \item[\say{$\Rarr$}] Se le due rette si intersecano significa che il sistema è compatibile, ovvero (Rouché-Capelli) \[3\geq \rk C=\rk A\implies \det A=0;\]
        se sono parallele, allora hanno giacitura uguale, per cui 
        \[\rk C=2\implies \rk A\leq 3\implies \det A=0\]
        \item[\say{$\Larr$}] Se il determinante della matrice è $0$, il rango è $2$ o $3$. Non può essere 1 perché almeno $2$ equazioni sono indipendenti per ipotesi ($r$ e $s$ sono rette). Di conseguenza
        \begin{itemize}
            \item se $\rk A=2$ significa che le due rette sono coincidenti;
            \item se $\rk A=3$ può essere:
            \begin{itemize}
                \item $\rk C=3$, allora le rette si intersecano in un punto;
                \item $\rk C=2$, le rette hanno la stesa giacitura ma non si intersecano, per cui sono parallele.
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Soluzioni degli esercizi}
\subsubsection*{Esercizio \ref{exc: simmetriche sse}}\label{sol: simmetriche sse}
\begin{proof}~
    \begin{itemize}
        \item [\say{$\Rarr$}] Assumiamo che la mappa sia bilineare simmetrica e dimostriamo che $A=A^t$. In particolare si ha che $f_A(x,y)=f_A(y,x)$. Osserviamo che se $k \in \K$ come $\K$ spazio vettoriale, $k^t=k$, quindi (ricordado che la trasposizione inverte l'ordine nel prodotto tra matrici)
        \[f_A(x,y)=[f_A(x,y)]^t=\left( x^t\cdot A \cdot y \right)^t= y^t\cdot  A^t \cdot \left( x^t \right)^t= y^t\cdot A^t \cdot x \] 
        Per ipotesi si ha che $f_A(x,y)=f_A(y,x)$, quindi 
        \[y^t\cdot A^t \cdot x= y^t\cdot A \cdot x\]
        da cui segue $A=A^t$.
        \item [\say{$\Larr$}] Assumiamo $A=A^t$ e dimostriamo che $f_A(x,y)=f_A(y,x)$. Per quanto detto prima $f_A(x,y)=y^t\cdot A^t\cdot x$. Inoltre per ipotesi $A=A^t$, quindi $f_A(x,y)=y^t\cdot A\cdot x=f_A(y,x)$.
    \end{itemize}
\end{proof}
La dimostrazione è analoga per matrici e mappe antisimmetriche.

\subsubsection*{Esercizio \ref{exc: sottosp perp}}\label{sol: sottosp perp}
\begin{proof}Per definizione di sottospazio vettoriale, $S^\perp$ deve essere
    \begin{itemize}
        \item non vuoto:
        \[0\in S^\perp\text{, infatti }b(0,w)=b(0\cdot 0,w)=0\cdot b(0,w)=0\]
        \item chiuso rispetto a somma e prodotto per scalari ($\forall v_1,v_2\in S^\perp\ \ \forall \lambda_1, \lambda_2\in \K$):\\
        Equivalentemente $b(v_1,w)=b(v_2,w)=0\ \ \forall w\in S\ \implies$
        \[b(\lambda_1v_1+\lambda_2v_2,w)= \lambda_1\underbrace{b(v_1,w)}_0+\lambda_2\underbrace{b(v_2,w)}_0= 0\]
    \end{itemize}
\end{proof}

\subsubsection*{Esercizio \ref{exc: lemma finestra}}\label{sol: lemma finestra}
\begin{proof}Le proprietà di $\psi$ seguono direttamente dalle proprietà di $b$:
    \begin{itemize}
        \item $\psi(v)$ è lineare $\forall v$
            \[\psi(v)(\lambda_1w_1+\lambda_2w_2)=b(v,\lambda_1w_1+\lambda_2w_2)= \lambda_1b(v,w_1)+\lambda_2b(v,w_2) = \lambda_1\psi(v)(w_1)+\lambda_2\psi(v)(w_2) \]
        \item  $\psi$ è lineare
        \[\psi(\lambda_1v_1+\lambda_2v_2)=\lambda_1\psi(v_1)+\lambda_2\psi(v_2)\]
        Si tratta di un'uguaglianza tra mappe, per cui è verificata se e solo se è verificata $\forall w$:
        \[\psi(\lambda_1v_1+\lambda_2v_2)(w)= b(\lambda_1v_1+\lambda_2v_2,w)= \lambda_1b(v_1,w)+\lambda_2b(v_2,w)= \lambda_1\psi(v_1)(w)+\lambda_2\psi(v_2)(w)\]
        \item $\psi$ è iniettiva, equivalentemente $\ker \psi =\{0\}$
        \\Se $\psi(v)$ è la mappa nulla, allora $v=0$, ovvero se $\forall w, \psi(v)(w)=0$, allora $v=0$.
        Prendiamo un qualsiasi $v,w\in W$. Sia $\beta=\{e_i\}_i$ una base di $V$ che diagonalizza $b$ (esiste perchè $b$ è simmetrica per ipotesi), allora
        \[v=\sum_ix_ie_i\qquad \qquad w=\sum_jy_je_j.\]
        Per linearità segue che
        \[\psi(v)(w)=b\left(\sum_ix_ie_i, \sum_jy_je_j\right)=\sum_i\sum_jx_iy_jb(e_i,e_j).\]
        Siccome la base $\beta$ diagonalizza $b$, segue che $i\neq j\implies b(e_i,e_j)=0$, quindi 
        \[\psi(v)(w)=\sum_ix_iy_ib(e_i,e_i).\]
        Inoltre, siccome la mappa $b$ è non degenere, per la legge di annullamento del prodotto, segue che $a_i=0\forall i$, quindi $v=0$. Per l'arbitrarietà di $w$, la mappa è iniettiva.
        \item $\psi$ è suriettiva: segue dal teorema Nullità + Rango.
    \end{itemize}
\end{proof}

\subsubsection*{Esercizio \ref{exc: sottosp aff-vett}}\label{sol: sottosp aff-vett}
\begin{proof}~
    \begin{enumerate}[label=$\roman*)$]
        \item $\forall v,w\in V,\exists!u\in V:\overline{vu}=w$. Osserviamo che basta definire $u:=v+w$ e segue che $\overline{vu}=u-v=(v+w)-v=w$.
        \item $\forall u,v,w\in V, \overline{uv}+\overline{vw}=\overline{uw}$. Applicando la definizione segue infatti che 
        \[\overline{uv}+\overline{vw}= (v-u)+(w-v)=w-u=\overline{uw}.\]
    \end{enumerate}
\end{proof}

\newpage
\section{Situazioni}
\printtheorems
\end{document}