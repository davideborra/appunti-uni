%Update 27/02/2023
% Changelog

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{article}     %type of document
\usepackage{lambdatex} 

\let\undl\underline

\newcommand{\gln}[2]{\mathpzc{Gl}_{#1}(#2)}


\makeatletter
\renewcommand*\env@cases[1][1.2]{%
  \let\@ifnextchar\new@ifnextchar
  \left\lbrace
  \def\arraystretch{#1}%
  \array{@{}l@{\quad}l@{}}%
}
\makeatother

\title{Geometria A}
\author{Davide Borra}
\date{}
\makeatletter
\let\runauthor\@author
\let\runtitle\@title
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\footrulewidth}{0.4pt}

\everymath{\displaystyle}

\begin{document}

\lhead{}
\chead{}
\rhead{Indice}
\rfoot{\runauthor}

\begin{titlepage}
    \newgeometry{left=3cm, right=3cm, bottom=2cm, top =3cm} 
    \pagestyle{empty}
    \begin{center}
        \vspace*{\fill}
        \vspace{0.5cm}
        \textbf{\Huge \runtitle}\\\textsc{secondo modulo}\\\vspace{5mm}
        \textsc{\Large \runauthor}
        \vspace{5cm}
    \end{center}
    \vspace*{\fill}
    v. 1.0\\
    \rule{0.8\linewidth}{0.5mm}\\
    {\footnotesize\href{mailto:davide.borra@studenti.unitn.it}{davide.borra@studenti.unitn.it} - \href{http://davideborra.github.io}{davideborra.github.io}}
    \restoregeometry\newpage
    \thispagestyle{empty}
    \begin{abstract}
        \centering Queste sono le note prodotte durante il secondo modulo del corso di Geometria A, tenuto dal prof.~Marco Andreatta. Il docente del corso segue il libro \say{Geometria 1} di Edoardo Sernesi (Ed. Bollati Boringhieri).
    \end{abstract}
    \tableofcontents
    \creativecommons
    
    \lhead{\runtitle}
    \chead{}
    \rhead{\rightmark}
    \rfoot{\runauthor}
\end{titlepage}


\section{Forme bilineari}
\begin{boxdef}[forma bilineare]
    Sia $V$ un $\K$-spazio vettoriale. Si dice forma bilineare una mappa lineare
    \[f:V\times V\to \K\]
    lineare rispetto ad entrambi gli argomenti, ovvero $\forall v_1, v_2, w_1, w_2 \in V$, $\forall k\in \K$
    \begin{tasks}(2)
        \task[\textbullet] $f(v_1+v_2, w_1)=f(v_1,w_1)+f(v_2,w_1)$
        \task[\textbullet] $f(v_1, w_1+w_2)=f(v_1,w_1)+f(v_1,w_2)$
        \task[\textbullet] $f(kv_1, w_1)=kf(v_1,w_1)$
        \task[\textbullet] $f(v_1, kw_1)=kf(v_1,w_1)$
    \end{tasks}
\end{boxdef}

Esistono inoltre alcune forme multilineari particolari:
\begin{boxdef}[Forme bilineari simmetriche]
    Sia $V$ un $\K$-spazio vettoriale. Una forma bilineare $f:V\times V \to \K$ si dice simmetrica se $\forall v, w\in V$
    \[f(v,w)=f(w,v)\]
\end{boxdef}
\begin{boxdef}[Forme bilineari antisimmetriche]
    Sia $V$ un $\K$-spazio vettoriale. Una forma bilineare $f:V\times V \to \K$ si dice antisimmetrica se $\forall v, w\in V$
    \[f(v,w)=-f(w,v)\]
\end{boxdef}
Data una matrice, ad essa è associata una forma bilineare del tipo $f_A(x,y)=x^t\cdot A \cdot y$. La dimostrazione del fatto che questa mappa sia bilineare segue dalla proprietà distributiva del prodotto tra matrici.
\begin{exc}\label{exc: simmetriche sse}
    Dimostrare che $f_A$ è simmetrica se e solo se $A$ è simmetrica, ovvero se e solo se $A=A^t$. 
    \qquad {\hyperref[sol: simmetriche sse]{\footnotesize Soluzione a pag. \pageref*{sol: simmetriche sse}}}
\end{exc}



\begin{shadedTheorem}[Matrice associata]
    Sia $V$ un $\K$-spazio vettoriale con dimensione $n$ finita e $\beta=\{e_1, \dots, e_n\}$ una base di $V$. Siano inoltre $f:V\times V \to \K$ una forma bilineare e $A=\left( f(e_i,e_j) \right)_{ij}\in M_{n\times n }(\K)$. Allora se $v=\sum_ia_ie_i$ e $w=\sum _jb_je_j$, si ha 
    \[f(v,w)=\left( a_1,\dots, a_n \right) \cdot A \cdot \begin{pmatrix}
        b_1\\\vdots\\ b_n
    \end{pmatrix}\]
\end{shadedTheorem}
\begin{proof}
    \[
    \begin{aligned}
        f(v,w)= f\left(\sum_ia_ie_i,\sum_ja_je_j\right)= \sum_ia_i\left( \sum_j b_jf(e_i,e_j) \right)
    \end{aligned}
    \]
    Osserviamo che 
    \[\left( a_1,\dots, a_n \right) \cdot A \cdot \begin{pmatrix}
        b_1\\\vdots\\ b_n
    \end{pmatrix} = \left( a_1,\dots, a_n \right)\begin{pmatrix}
        \sum_j b_jf(e_1,e_j)\\ \vdots \\ \sum_j b_jf(e_n,e_j)
    \end{pmatrix}  = \sum_ia_i\left( \sum_j b_jf(e_i,e_j)\right)\]
    da cui la tesi.
\end{proof}
\begin{oss}
    Scelta una base esiste una corrispondenza biunivoca tra le forme bilineari e le matrici e una corrispondenza biunivoca tra le forme bilineari simmetriche e le matrici simmetriche.
\end{oss}
\subsection{Matrici simili}
\begin{lemma}[Matrici simili]
    Siano $\beta=\{e_1, \dots, e_n\}$ e $\beta'=\{u_1, \dots, u_n\}$ due basi di $V$, $\K$-spazio vettoriale. Sia $f_V\times V\to \K$ una forma bilineare e siano $A=(f(e_i,e_j))_{ij}$ e $A=(f(u_i,u_j))_{ij}$ le matrici associate alla mappa $f$ in $\beta $ e $\beta'$ rispettivamente. Allora $\exists M=M_{\beta\beta'}(id)\in \gln{n}{\K}\ :\ B=M^t AM$
\end{lemma}
\paragraph{Notazione} se $v= \sum_ix_ie_i= \sum _ix'_iu_i$, allora $\underline x = (v)_\beta$ e $\underline x' = (v)_\beta'$. Analogamente per $w$ indicando le coordinate con $y$.
\begin{proof}
    \[\underline{x} = M\undl x'\qquad \qquad \undl y = M \undl y'\]
    da cui 
    \[f(v,w)= \undl x^tA \undl y = (M\undl x)^t A (M\undl y) = ( \undl x')^tM^tAM\undl y = ( \undl x')^t B \undl y\]
    quindi $B=M^tAM$
\end{proof}
\begin{boxdef}[Matrici congruenti]
    Due matrici $A,B\in M_{n\times n}(\K)$ si dicono congruenti se $\exists M \in \gln{n}{K}:$
    \[B=M^tAM\]
\end{boxdef}

\begin{boxdef}[Base ortogonale]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$, e $\beta$ una base di $V$. Se $i\neq j\implies b(e_i,e_j)=0$ , la base si dice diagonalizzante o ortogonale per la forma bilineare $b$. 
\end{boxdef}
\begin{oss}
    Se la base è ortogonale, allora la matrice associate $A=(b(e_i,e_j))_{ij}$ è diagonale.
\end{oss}

\begin{boxdef}[Forma quadratica associata]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$, $b: V\times V \to \K$ bilineare simmetrica. Si definisce la forma quadratica associata
    \[\funcdef{q}{V}{\K}{v}{b(v,v)}\]
\end{boxdef}
\begin{oss}
    Non è lineare.
\end{oss}
\begin{ricorda}
    Si dice forma una mappa ad un campo, non necessariamente lineare. 
\end{ricorda}
\paragraph*{Proprietà} (seguono dalla linearità di $b$)
\begin{enumerate}[label={\it\roman*})]
    \item $q(\lambda v)=\lambda^2q(v)$
    \item $2b(v,w)=q(w+v)-q(v)-q(w)$
\end{enumerate}
La proprietà (\textit{ii})è importante perché permette di definire $b$ usando $q$ e viceversa.

\begin{oss}
    Consideriamo una base $\beta=\{e_1,\dots, e_n\}$ di $V$, allora un vettore generico si esprime in coordinate come 
    \[v=\sum_ix_ie_i\]
    da cui 
    \[b(v,w)= \begin{pmatrix}
        x_1&\cdots&x_n
    \end{pmatrix} \begin{pmatrix}
        a_{ij}
    \end{pmatrix}_{ij}\begin{pmatrix}
        x_1\\\vdots\\x_n
    \end{pmatrix}=\sum_{ij}a_{ij}x_ix_j\]
    per cui in coordinate una forma quadratica è un polinomio omogeneo di secondo grado nelle variabili $x_i$ (è importante ricordare che per ipotesi $a_{ij}=a_{ji}\ \forall ij$).
\end{oss}

\begin{ex}[Consideriamo $\R^2$ con la base canonica e la forma quadratica 
    \[q(x_1,x_2)=3x_1^2-2x_1x_2-x_2^2\]]
    \[q(x_1,x_2)=3x_1^2-2x_1x_2-x_2^2=3x_1^2-x_1x_2-x_2x_1-x_2^2= \begin{pmatrix}
        x_1&x_2
    \end{pmatrix} \begin{pmatrix}
        3&-1\\-1&-1
    \end{pmatrix} \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix} \]
    Essa è quindi associata alla forma bilineare 
    \[b\left( \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix},\begin{pmatrix}
        x_1\\x_2
    \end{pmatrix}\right) = \begin{pmatrix}
        x_1&x_2
    \end{pmatrix} \begin{pmatrix}
        3&-1\\-1&-1
    \end{pmatrix} \begin{pmatrix}
        x_1\\x_2
    \end{pmatrix} \]
\end{ex}

\begin{boxdef}[Vettori isotropi]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$ e $b(v,w)$ una forma bilineare simmetrica. Un vettore $v\in V$ si dice isotropo se 
    \[b(v,v)=q(v)=0\]
\end{boxdef}

\begin{boxdef}[Spazio ortogonale]
    Siano $V$ un $\K$-spazio vettoriale, $S\subseteq V$, $\dim V<\infty$ e $b(v,w)$ una forma bilineare simmetrica. Si definisce sottospazio perpendicolare a $S$ l'insieme 
    \[S^\perp:=\{v\in V\,|\,b(v,w)=0\ \forall w \in S\}\]
\end{boxdef}
\begin{exc}
    Dimostrare che $S^\perp$ è un sottospazio vettoriale di $V$.
\end{exc}
\begin{lemma}
    Siano $V$ un $\K$-spazio vettoriale, $S\subseteq V$, $\dim V<\infty$. Sia inoltre $v\in V$ non isotropo. Allora \[\langle v \rangle \oplus v^\perp = V\]
\end{lemma}
\begin{proof}
    Prendo un qualsiasi $w\in V$, allora sottraggo a $w$ la sua componente lungo $v$ e dimostro che appartiene a $v^\perp$, infatti 
    \[b\left( w-\frac{b(w,v)}{b(v,v)}v,v \right)= b(w,v)-b\left(\frac{b(w,v)}{b(v,v)}v,v\right)= b(w,v)-\frac{b(w,v)}{b(v,v)}b(v,v)=0\]
    quindi 
    \[ w-\frac{b(w,v)}{b(v,v)}v\in v^\perp\]
    Allora posso scrivere $w$ come la somma di un vettore in $\langle v\rangle$ e un vettore in $v^\perp$:
    \[w=\underbrace{\frac{b(w,v)}{b(v,v)}v}_{\in\langle v\rangle}+\underbrace{\left( w-\frac{b(w,v)}{b(v,v)}v \right)}_{\in v^\perp}\]
    di conseguenza \(\langle v \rangle + v^\perp = V\). Inoltre osserviamo che per costruzione di $v^\perp$, \(\langle v \rangle \cap v^\perp = \{0\}\), quindi la somma è diretta.
\end{proof}

\begin{shadedTheorem}[Diagonalizzabilità di forme bilineari]
    Siano $V$ un $\K$-spazio vettoriale, $\dim V<\infty$. Se $b$ è una forma bilineare simmetrica, allora esiste una base ortogonale per $b$, ovvero esiste una matrice $M\in \gln{n}{\K}$ tale che $M^tAM$ è diagonale.
\end{shadedTheorem}

\begin{proof}
    Se $b(v,w)$ è identicamente nulla, il teorema vale. Altrimenti possiamo supporre che $\exists v,w\in V: b(v,w)\neq 0$. Esiste quindi un vettore non isotropo, infatti 
    \begin{itemize}
        \item se $b(v,v)\neq 0$, è $v$;
        \item se $b(w,w)\neq 0$, è $w$;
        \item altrimenti, se $b(v,v)=b(w,w)=0$, $b(v+w,v+w)=b(v,v)+b(w,w)+2b(v,w)=2b(v,w)\neq 0$, per cui il vettore non isotropo é $v+w$.
    \end{itemize}
Procediamo ora per induzione su $n=\dim V$:
\begin{itemize}
    \item[$\N0)$] Se $n=1$ il teorema è vero (una matrice $1\times 1$ è diagonale)
    \item[$\N1)$] Supponiamo vero il teorema per $\dim V=n$, dimostriamo che3 vale per $\dim V = n+1$, ovvero che esiste una base diagonalizzante per ogni matrice simmetrica e per ogni spazio vettoriale di dimensione $n$. Prendiamo quindi un vettore $e_1$ non isotropo. Definiamo quindi il suo spazio perpendicolare, che (per il lemma precedente e la formula di Grassman) ha dimensione $n$. Per ipotesi induttiva esiste quindi una base ortogonale $\gamma=\{e_2, \dots, e_n\}$ di $e_1^\perp$ per $b|_{e_1^\perp}$, per cui la base cercata è \[\beta=\gamma \cup \{e_1\}=  \{e_1, \dots, e_n\}\]
\end{itemize}
\end{proof}

\begin{ex}
    [Sia $V=\R^3$ e $\beta = \{e_1,e_2,e_3\}$ la base canonica di $V$. In questa base sia $q(x,y,z)=xy+xz+yz$ una forma quadratica: diagonalizzarla.]
    Non è presente il termine $x^2$ per cui dobbiamo fare in modo di ricavarlo. Poniamo quindi
    \[\begin{cases}
        x'=x\\
        y'=y+x\\
        z'=z
    \end{cases}\qquad \qquad \begin{cases}
        x=x'\\
        y=y'-x'\\
        z=z'
    \end{cases}\]
    Sostituiamo e completiamo il quadrato
    \[q(x',y',z')=x'(y'-x')+x'z'+(y'-x')z'= -{x'}^2+x'y'+y'z'=-\left(x'-\frac{1}{2}y'\right)^2+\frac{1}{4}{y'}^2+y'z'\]
    Cambiamo coordinate
    \[\begin{cases}
        x''=x'-\frac{1}{2}y'\\
        y''=y'\\
        z''=z'
    \end{cases}\qquad \qquad \begin{cases}
        x'=x''+\frac{1}{2}y''\\
        y'=y''\\
        z'=z''
    \end{cases}\]
    \[q(x'',y'',z'')=-{x''}^2+\frac{1}{4}{y''}^2+y''z''=-{x''}^2+\left( \frac{1}{2}y''+z'' \right)^2-{z''}^2\]
    \[\begin{cases}
        x'''=x''\\
        y'''=\frac{1}{2}y''+z''\\
        z'''=z''
    \end{cases}\qquad \qquad \begin{cases}
        x''=x'''\\
        y''=2y'''-2z'''\\
        z''=z'''
    \end{cases}\]
    \[q(x''',y''',z''')=-{x'''}^2+{y'''}^2-{z'''}^2\]

    Ricaviamo ora le matrici di cambio di base:
    \[\begin{aligned}
        [I]\qquad \qquad &
        \begin{pmatrix}
            1&0&0\\
            -1&1&0\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x'\\y'\\z'
        \end{pmatrix}= \begin{pmatrix}
            x\\y\\z
        \end{pmatrix}\\
    [II]\qquad \qquad &
        \begin{pmatrix}
            1&\frac{1}{2}&0\\
            0&1&0\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x''\\y''\\z''
        \end{pmatrix}= \begin{pmatrix}
            x'\\y'\\z'
        \end{pmatrix}\\
    [III]\qquad \qquad &
        \begin{pmatrix}
            1&0&0\\
            0&2&-2\\
            0&0&1
        \end{pmatrix}\begin{pmatrix}
            x'''\\y'''\\z'''
        \end{pmatrix}= \begin{pmatrix}
            x''\\y''\\z''
        \end{pmatrix}
    \end{aligned}
    \]
    quindi
    \[\underbrace{\begin{pmatrix}
        1&0&0\\
        -1&1&0\\
        0&0&1
    \end{pmatrix}
    \begin{pmatrix}
        1&\frac{1}{2}&0\\
        0&1&0\\
        0&0&1
    \end{pmatrix}
    \begin{pmatrix}
        1&0&0\\
        0&2&-2\\
        0&0&1
    \end{pmatrix}}_{M}\begin{pmatrix}
        x'''\\y'''\\z'''
    \end{pmatrix}= \begin{pmatrix}
        x\\y\\z
    \end{pmatrix}\]
    da cui
    \[{\renewcommand{\arraystretch}{2}
        M^t\cdot\begin{pmatrix}
        0&\frac{1}{2}&\frac{1}{2}\\
        \frac{1}{2}&0&\frac{1}{2}\\
        \frac{1}{2}&\frac{1}{2}&0
    \end{pmatrix}\cdot M}=\begin{pmatrix}-1&0&0\\0&1&0\\0&0&-1\end{pmatrix}\]
\end{ex}

\begin{shadedTheorem}[Sylvester I]
    Sia $\K$ un campo algebricamente chiuso. Sia $V$ un $\K$-spazio vettoriale con $\dim V=n\geq 1$. Sia $b:V\times V\to \K$ una forma bilineare simmetrica. Allora esiste una base 
    \[\beta=\{u_1, \dots, u_n\}\]
    tale che 
    \[\begin{array}{rcl}
        i\neq j & \implies  &b(u_i,u_j)=0;\\
        1\leq i\leq r\leq n &\implies &b(u_i,u_i)=1;\\
        r<i\leq n& \implies & b(u_i,u_i)=0.
    \end{array}\]
    Equivalentemente la forma quadratica rispetto a $\beta$ è 
    \[q(x_1, \dots, x_n)=x_1^2+\dots+ x_r^2\]
\end{shadedTheorem}
\begin{proof}
    Per il teorema precedente, esiste una abse ortogonale $\gamma=\{e_i\}_i$. Riordinando possiamo supporre che 
    \[\begin{array}{rcl}
        1\leq i\leq r\leq n &\implies &b(e_i,e_i)\neq 0;\\
        r<i\leq n& \implies & b(e_i,e_i)=0.
    \end{array}\]
    Poniamo 
    \[\begin{cases}
        u_i:=\frac{e_i}{\sqrt{b(e_i,e_i)}}& i=1,\dots, r\\
        u_i:=e_i& i=r+1, \dots, n
    \end{cases}\]
    Otteniamo che $\beta=\{u_i\}_i$ è la base cercata.
\end{proof}

\begin{shadedTheorem}[Sylvester II]
    Siano $V$ un $\R$-spazio vettoriale e $b:V\times V\to \R$ una forma bilineare simmetrica. Allora esiste una base 
    \[\beta=\{u_2, \dots, u_n\}\]
    tale che 
    \[\begin{array}{rcl}
        i\neq j & \implies  &b(u_i,u_j)=0;\\
        1\leq i\leq r\leq t\leq n &\implies &b(u_i,u_i)=1;\\
        1\leq r< i\leq t\leq n &\implies &b(u_i,u_i)=-1;\\
        t<i\leq n& \implies & b(u_i,u_i)=0.
    \end{array}\]
\end{shadedTheorem}
La dimostrazione è analoga al teorema precedente ponendo 
\[\begin{cases}
    u_i:=\frac{e_i}{\sqrt{|b(e_i,e_i)|}}& i=1,\dots, r\\
    u_i:=e_i& i=r+1, \dots, n
\end{cases}\]
e riordinando opportunamente.

Un'osservazione interessante è che il numero di 0, di 1 e di -1 non dipende dalla scelta di una base ortogonale.
\begin{shadedTheorem}[Rango]
    ($V$ $\K$-spazio vettoriale, $\dim V=n$) Data una forma bilineare simmetrica $b:V\times V \to \K$ e una sua base diagonalizzante $\beta=\{e_i\}_i$, il numero di vettori $e_i$ tali che $b(e_i,e_i)=0$ non dipende dalla scelta della base diagonalizzante. Esso è uguale al rango di ogni matrice associata.
\end{shadedTheorem}
\begin{proof}
    Ricordiamo il seguente lemma:
    \begin{lemma}
        Siano $A\in M{n\times m}(\K)$, $N\in \gln{n}{\K}$ e $M\in \gln{m}{\K}$, allora $\rk\left( NAM \right)=\rk A$.
    \end{lemma}
    Siano $\beta$ e $\beta'$ due basi, $M=M_{\beta\beta'}(Id)$, $A=M_\beta(b)$ e $B=M_{\beta'}(b)$. Allora per dimostrazione precedente si ha $B=M^tAM$. Per il lemma appena enunciato segue che, siccome $M\in \gln{n}{\K}$, $\rk B=\rk A$.
    Supponiamo che esistano due basi $\beta=\{e_i\}_i$ e $\beta'=\{u_i\}_i$ diverse tali che (numeri di 1 e -1 diversi).
    \begin{itemize}
        \item \[\begin{array}{rcl}
            i\neq j & \implies  &b(e_i,e_j)=0;\\
            1\leq i\leq t\leq r\leq n &\implies &b(e_i,e_i)=1;\\
            1\leq t< i\leq r\leq n &\implies &b(e_i,e_i)=-1;\\
            r<i\leq n& \implies & b(e_i,e_i)=0.
        \end{array}\]
        \item \[\begin{array}{rcl}
            i\neq j & \implies  &b(u_i,u_j)=0;\\
            1\leq i\leq s\leq r\leq n &\implies &b(u_i,u_i)=1;\\
            1\leq s< i\leq r\leq n &\implies &b(u_i,u_i)=-1;\\
            r<i\leq n& \implies & b(u_i,u_i)=0.
        \end{array}\]
    \end{itemize}
    con $s\neq t$. È lecito supporre che $t>s$ (altrimenti scegliamo al posto di $U$ e $W$ i rispettivi complementi diretti a $V$). Siano $U=\langle e_1, \dots, e_t\rangle$ e $W=\langle u_{s+1},\dots, u_n\rangle$. Segue quindi che $\dim U=t$ e $\dim W=n-s$, da cui 
    \[\dim U+\dim W=n-s+t>n,\]
    quindi per la formula di Grassman segue che $\dim U\cap V>0$. Prendiamo quindi un vettore non nullo $v\in U\cap V$.
    \[v=\sum_{i=1}^ta_ie_i=\sum_{i=s+1}^nb_iu_i\] 
    per cui
    \[\begin{aligned}
        b(v,v)&=\sum_{i=1}^t\underbrace{b(e_i,e_i)}_1a_i^2= \sum_{i=1}^ta_i^2>0\\
        &=\sum_{i=s+1}^n\underbrace{b(u_i,u_i)}_{-1}b_i^2= -\sum_{i=s+1}^nb_i^2<0\\
    \end{aligned}.\]
    Si presenta quindi un assurdo, per cui deve essere che $s=t$.
\end{proof}
\begin{boxdef}[Rango]
    ($V$ $\K$-spazio vettoriale, $\dim V=n$) Si definisce rango di una forma bilineare simmetrica $b:V\times V\to \K$ il numero di vettori $e_i$ di una base diagonalizzante tali che $b(e_i,e_i)\neq 0$. Esso si indica $\rk b$. Se $\rk b=\dim V$, allora la forma si dice non degenere.
\end{boxdef}
\begin{boxdef}[Segnatura]
    Siano $V$ un $\R$-spazio vettoriale, $b:V\times V\to \K$ una forma bilineare simmetrica e $\beta=\{e_i\}_i$ una base diagonalizzante normalizzata. 
    Allora si definiscono
    \begin{itemize}
        \item \textbf{indice di positività:} $t=\#\{e\in \beta\,|\, b(e,e)=1\}$
        \item \textbf{indice di negatività:} $s=\#\{e\in \beta\,|\, b(e,e)=-1\}$
    \end{itemize}
    La coppia $(t,s)$ è detta segnatura di $b$.
\end{boxdef}
\begin{boxdef}[Forme definite positive/negative]
    ($V$ $\R$-spazio vettoriale, $\dim V=n$) Una forma quadratica $q$ su $V$ si dice:
    \begin{itemize}
        \item \textbf{definita positiva} se $q(v)>0\ \ \forall\,v\in V $
        \item \textbf{definita negativa} se $q(v)<0\ \ \forall\,v\in V $
        \item \textbf{semidefinita positiva} se $q(v)\geq0\ \ \forall\,v\in V $
        \item \textbf{semidefinita negativa} se $q(v)\leq0\ \ \forall\,v\in V $ 
    \end{itemize}
\end{boxdef}
In particolare lo è se lo è su una base.
\begin{lemma}[Finestra sul mondo duale]
    Siano $V$ un $\K$-spazio vettoriale, $b:V\times V\to \K$ una forma bilineare simmetrica non degenere, allora esiste un isomorfismo
    \[\begin{array}{ccccccl}            \psi : & V & \longrightarrow & V^\vee \\                 & v & \longmapsto     & \psi(v) : & V & \longrightarrow & \mathbb{K} \\&&&                 & w & \longmapsto     &\psi(v)(w):=b(v,w)       \end{array}\]
\end{lemma}
%\vartriangleleft per sottospazi vettoriali

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Soluzioni degli esercizi}
\subsubsection*{Esercizio \ref{exc: simmetriche sse}}\label{sol: simmetriche sse}
\begin{proof}~
    \begin{itemize}
        \item [\say{$\Rarr$}] Assumiamo che la mappa sia bilineare simmetrica e dimostriamo che $A=A^t$. In particolare si ha che $f_A(x,y)=f_A(y,x)$. Osserviamo che se $k \in \K$ come $\K$ spazio vettoriale, $k^t=k$, quindi (ricordado che la trasposizione inverte l'ordine nel prodotto tra matrici)
        \[f_A(x,y)=[f_A(x,y)]^t=\left( x^t\cdot A \cdot y \right)^t= y^t\cdot  A^t \cdot \left( x^t \right)^t= y^t\cdot A^t \cdot x \] 
        Per ipotesi si ha che $f_A(x,y)=f_A(y,x)$, quindi 
        \[y^t\cdot A^t \cdot x= y^t\cdot A \cdot x\]
        da cui segue $A=A^t$.
        \item [\say{$\Larr$}] Assumiamo $A=A^t$ e dimostriamo che $f_A(x,y)=f_A(y,x)$. Per quanto detto prima $f_A(x,y)=y^t\cdot A^t\cdot x$. Inoltre per ipotesi $A=A^t$, quindi $f_A(x,y)=y^t\cdot A\cdot x=f_A(y,x)$.
    \end{itemize}
\end{proof}
La dimostrazione è analoga per matrici e mappe antisimmetriche.
\end{document}